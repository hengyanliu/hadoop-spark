   <pre>
   相信大家看了我博客前面的文章，对线性回归中的数学思想应该有所了解如果弄懂了这其中所蕴含的数学公式，代码很容易看得懂.
 机器学习中的线性回归算法的种类有很多，例如神经网络回归算法，蚁群回归算法，支持向量机回归算法等，这些都可以在一定程
 度上达到回归拟合的目的。MLlib中使用的是较为经典的随机梯度下降算法，它充分利用了Spark框架的迭代计算的特性通过不断判
 断和选择当前目标下的最优路径，从而能在最短路径下达到最优的结果，继而提高大数据的计算效率。
    结合实战源码，以及对源码的解读，我们来进一步学习MLlib计算框架中最核心的梯度下降算法。
我们用以下数据训练模型：
5,1 1
7,2 1
9,3 2
11,4 1
19,5 3
18,6 2
其中y和x1、x2以逗号隔开,x1与x2以空格隔开，预测的函数形式为y=a*x1+b*x2，
其次是对MLlib算法中数据格式进行处理，从源码中我们可以看到：
</pre>

![](https://github.com/woshidandan/hadoop-spark/blob/master/picture/sgd18.png)

其中的主要参数即输入的RDD，迭代次数，步长即训练的速度。其中，我们首先要对我们获得的数据进行分片处理，如代码中相关的程序段所示：
```scala
import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.regression.{LabeledPoint, LinearRegressionWithSGD}
import org.apache.spark.{SparkConf, SparkContext}

/**
  * Created by 2281444815 on 2017/1/22.
  */
object LinearRegression {
  val conf = new SparkConf().setMaster("local").setAppName("LinearRegression");
  val sc = new SparkContext(conf);

  def main(args: Array[String]): Unit = {
    val data = sc.textFile("D:\\data\\LinearRegression.txt")
    //对数据分片处理
    val parsedData = data.map{line =>
      val parts = line.split(",");
      LabeledPoint(parts(0).toDouble,   Vectors.dense(parts(1).split(" ").map(_.toDouble)));
    }.cache();

    //传入训练参数
    val model = LinearRegressionWithSGD.train(parsedData,20,0.1);

    //对我们所给的指定x1和x2进行y的预测
    val result = model.predict(Vectors.dense(1,1));

    println("参数的值为："+ model.weights);
    println("预测的结果为:" + result);

    val valuesAndPreds = parsedData.map { point =>{
      val prediction = model.predict(point.features)
      (point.label,prediction)
    }
    }
    val MSE = valuesAndPreds.map{
      case(v,p) => math.pow((v - p),2)}.mean()
    println("均方误差MSE为："+ MSE);
  }
}
```
我们在Debug模式中，可进一步探究代码的运行步骤，在到达这一步时：

![](https://github.com/woshidandan/hadoop-spark/blob/master/picture/sgd19.png)

是否感到很熟悉，这正是我们在解读数学思想时，里面的误差函数J(θ)，这也是整个线性回归中最重要的数学思想了。
代码中，我们用均方误差(Mean Squared Error,MSE)来衡量参数估计值与参数真实值的差距大小，MSE可以评价数据的变化程度，
MSE的值越小，说明预测模型描述实验数据具有更好的精确度。我们代码的运行结果如下：

![](https://github.com/woshidandan/hadoop-spark/blob/master/picture/sgd20.png)

即我们的函数近似可认为在均方误差为1.28的情况下，函数的形式为：y=2.6*x1+1.4*x2.
那么到这里，我们好像这个算法也到此结束了，但是，我们的学习并不是浅尝辄止，并不是对网上已有的资料照搬照抄，我们要有
自己的思考，自己的实践，这样，我们才能在同等的资源和同样的时间里学得更多，就我们的测试数据来说，如果我们修改测试数据
中的一项，如下所示：
<pre>
500,100 100
7,2 1
9,3 2
11,4 1
19,5 3
18,6 2
</pre>
就我们的数学常识来说，相当于把y和x同乘以100，对预测的结果不应该产生任何的影响，而我们实际的运行结果呢？

![](https://github.com/woshidandan/hadoop-spark/blob/master/picture/sgd21.png)

为什么会出现这种情况，预测的参数值竟然为负值？
如果我们进一步思考其中的数学思想，相信并不难理解

![](https://github.com/woshidandan/hadoop-spark/blob/master/picture/sgd3.png)

我们的参数是以这个公式来更新的，也就是站在山顶，以最快的方向下山，然而，如果我们的数据，与其他数据设置的相差过远，或者
我们的训练数据过少，那么在这个特殊数据的那点，相当于我们有一个很长很陡的坡，但是我们的步长过小，导致在这点下山时，会出
现严重的问题，这点的作用，是体现在J(θ)里，从而影响参数的更新，最终影响都我们参数的预测。如果我们保持测试的数据不变，修
改步长0.1为其他值，也会出现这样的情况。这也告诉我们，在实际运用该算法时，我们的训练数据并不是随意的。
 当然，除了MLlib本身提供的算法外，我们当然也可以自己来编写SGD算法，代码如下：
 
```scala
import scala.collection.mutable

/**
  * Created by 2281444815 on 2016/12/7.
  */
object SGD {
  val data = mutable.HashMap[Int,Int]()
  def getData():mutable.HashMap[Int,Int] = {
    for(i <- 1 to 10){
      data += (i -> (12*i))
    }
    data
  }
  var Θ:Double = 0
  var α:Double = 0.1
  def sgd(x:Double ,y:Double) = {
    Θ = Θ-α*((Θ*x)-y)
  }

  def main(args: Array[String]): Unit = {
    val dataSource = getData()
    dataSource.foreach(myMap =>{
      sgd(myMap._1,myMap._2)
      println("最终结果Θ值为"+Θ)
    })
    println("最终结果Θ值为"+Θ)
  }
}
后续的学习，我也会不断更新我对这个算法的认识和了解

  如果你觉得我的文章对你有帮助，欢迎到我的博客留言区留言交流，我会虚心听取大家的意见和建议，为进一</br>
步的学习做调整。更多的算法解析，我也会根据自己的学习在我的博客发布，欢迎来访www.xiaohegithub.cn</br>
                                              2017/1/22

