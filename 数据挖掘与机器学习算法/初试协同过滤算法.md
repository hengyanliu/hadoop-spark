<h2>协同过滤</h2>
   在学习这个算法的时候，和以前不一样，因为它涉及的数学以及算法层次，比我以前学的算法都要深，很多东西，在我现在的
知识水平内，即使弄懂了，但是也不是很明白为什么要去这么做。因此，我花了很长的时间去学习这个算法，但是我后来逐渐的明
白了，我现在为什么要去学这些东西？我暂时既不要工作，也不要用它们作于研究，我只是兴趣而已，我只要能明白这里面的思想，
处理问题的思路，并且能够用MLlib中提供的简单明了的接口，来对我的那几个数据分析，产生让我明白的答案就好了，毕竟这些算
法，都是前人历经千辛万苦，调优改良而来，如果我抱着急功近利的心态，像网上很多的博客那样，说一些打太极拳的东西骗自己，
我觉得我心里无法过意的去，更别说拿出来丢人显眼了。因此，我对今后的学习，也有了进一步的了解，我想学的，我想写的，我想
表达出来给别人看的，都是我感兴趣的，我能理解的，仅此而已。下面切如正题：
   协同过滤常常被用于分辨某位特定顾客可能感兴趣的东西，这些结论来自于对其他相似顾客对哪些产品感兴趣的分析。协同过滤
以其出色的速度和健壮性，在全球互联网领域炙手可热。
   之前一直记不住这个算法的名字，后来我想了想，它为什么叫这个名字？首先，协同，说明这个算法是不能孤立存在的，就像它
里面推荐给用户的商品，都是无法脱离该用户对除此之外的商品的评价以及其他用户对商品的评价，也即商品之间的评价，协同合作，
给出最后的评价。那么这里的过滤是什么意思呢，这里我理解成，通过已有数据，过滤掉难以分析的特征值，过滤同类，留下用户最想
看到的，最想用的元素。
    在这里，我先想直接说算法的代码，而不是这其中的原理和数学思想，之后我们再来说为什么要用那些数学公式或者思想。
    这是我们的数据集：
1 11 1
1 12 1
1 13 1
1 14 0
1 15 1
2 11 1
2 12 2
2 13 2
2 14 5
2 15 4
3 11 2
3 12 3
3 13 1
3 14 5
3 15 1
4 11 1
4 12 2
4 13 2
4 14 5
4 15 4
5 11 1
5 12 1
5 13 1
5 14 5
5 15 1
因为用户和商品的命名如果涉及到中英文名字，不好处理，这里我们第一列用1、2、3、4、5来分别代表用户的名字，用第二列来表示某一种
商品的名称，第三列自然是用户对商品的评价分了，以下是网上找的代码，结合了我对源码的分析：
```scala

import org.apache.spark.mllib.recommendation.{ALS, Rating}
import org.apache.spark.{SparkConf, SparkContext}

/**
  * Created by 2281444815 on 2016/12/2.
  */
object ALSTest {
  def main(args: Array[String]): Unit = {
    val conf = new SparkConf().setMaster("local").setAppName("ALSTest");
    val sc = new SparkContext(conf);
    val data = sc.textFile("D:\\data\\alstest.txt");
    val ratings = data.map(_.split(" ") match {
      case Array(user,item,rate) =>    //转化数据集
        Rating(user.toInt,item.toInt,rate.toDouble)  //将数据转化为专用Rating
    })
    val rank = 2;//秩，这部分会在之后解释
    val numIterations = 4;
    val model = ALS.train(ratings,rank,numIterations,0.01); //进行模型训练，0.01为正则化参数，防止过拟合

    //为用户1推荐一个物品
    val rs = model.recommendProducts(1,1);
    rs.foreach(println)

   //预测结果
    //从 ratings 中获得只包含用户和商品的数据集
    val usersProducts = ratings.map{
      case Rating(user,product,rate)=>
        (user,product)
    }

    /**使用推荐模型对用户商品进行预测评分，得到预测评分的数据集
      * 这里面的predict方法，输入： 获得一组已知的用户和物品的数据对，返回: 数据对和预测的评分组成的对应关系
      * Predict the rating of many users for many products.
      * The output RDD has an element per each element in the input RDD (including all duplicates)
      * unless a user or product is missing in the training set.
      *
      * @param usersProducts  RDD of (user, product) pairs.
      * @return RDD of Ratings.
      */
    val predictions = model.predict(usersProducts).map{
      case Rating(user,product,rate) =>
        ((user,product),rate)
    }

    /**join方法，将真实评分数据集与预测评分数据集进行合并
      * 即将原本数据中用户物品数据对与评分组成的对应关系，加入到预测的对应关系中，相当于最终的数据集中
      * 用两个部分，一个是真实的用户物品与评分数据，一个是预测的用户物品与评分数据
      * Return an RDD containing all pairs of elements with matching keys in `this` and `other`. Each
      * pair of elements will be returned as a (k, (v1, v2)) tuple, where (k, v1) is in `this` and
      * (k, v2) is in `other`. Uses the given Partitioner to partition the output RDD.
      */
    val ratesAndPres = ratings.map{
      case Rating(user,product,rate)=>
        ((user,product),rate)
    }.join(predictions)

   //计算均方差
    val MSE = ratesAndPres.map{
      case ((user,product),(r1,r2))=>
        val err = (r1-r2)
        err*err
    }.mean()
    println("Mean Squared Error = "+MSE)
  }
}
```scala
```scala

部分问题我已经在上面的代码里解释过，但还是有很多东西需要进行进一步解释：
（1）隐藏因子rank的值，很多教程中对该值草草带过，没有进行任何的说明，而我发现，这个值对最后的预测结果有着很重大的影响，
我会在之后解释这个值的意思。
（2）如果我们反复的运行这个程序，让它推荐用户1一件物品，你会惊讶的发现，会有两个结果，即使你将迭代的次数增加，
还是会出现两个结果，这也是我在数据集中为什么将 用户1对其他四件物品的评分设为1，一件物品的评分设为0；而用户5的四件物品评分设为1，
一件物品的评分设为5的原因，我会在之后说明我的用意。
综合上述的两个问题，这时候，我们有必要，也是必须来引入ALS算法中的数学思想了,我从网上摘下了对其中数学思想解释的部分内容：
![](https://github.com/woshidandan/hadoop-spark/blob/master/picture/als1.jpg)
![](https://github.com/woshidandan/hadoop-spark/blob/master/picture/als2.jpg)
在这张图片中，我们可以看到，我们有三个特征值：性格、文化程度、兴趣爱好，这三个值，也即对应了我们模型训练中，隐藏因子rank的值，当然，
我们上面的代码中，为了测试方面，将特征值的数量设为了2。这也是我经过学习之后，发现该算法的一个不足之处，因为在实际的学习或者实践中，
我们很难确定我们的特征值的数量，有的人看电影可能只是看特效给评分，有的人看电影，会综合电影的很多因素给出评分，而我们的算法，力求考虑到
整个数据集考虑到的方方面面，来对我们的个体进行分析，这样很可能造成以全概偏，对最后的结果造成严重的误差，所以，如何确定特征值rank的大小，
是要根据经验而来，还是根据我们的数据而来，又是怎么来的，我不得而知，只能通过将来的学习来了解这个问题的解决思路。
  
