<h2>协同过滤</h2>
   在学习这个算法的时候，和以前不一样，因为它涉及的数学以及算法层次，比我以前学的算法都要深，很多东西，在我现在的
知识水平内，即使弄懂了，但是也不是很明白为什么要去这么做。因此，我花了很长的时间去学习这个算法，但是我后来逐渐的明
白了，我现在为什么要去学这些东西？我暂时既不要工作，也不要用它们作于研究，我只是兴趣而已，我只要能明白这里面的思想，
处理问题的思路，并且能够用MLlib中提供的简单明了的接口，来对我的那几个数据分析，产生让我明白的答案就好了，毕竟这些算
法，都是前人历经千辛万苦，调优改良而来，如果我抱着急功近利的心态，像网上很多的博客那样，说一些打太极拳的东西骗自己，
我觉得我心里无法过意的去，更别说拿出来丢人显眼了。因此，我对今后的学习，也有了进一步的了解，我想学的，我想写的，我想
表达出来给别人看的，都是我感兴趣的，我能理解的，仅此而已。下面切如正题：
   协同过滤常常被用于分辨某位特定顾客可能感兴趣的东西，这些结论来自于对其他相似顾客对哪些产品感兴趣的分析。协同过滤
以其出色的速度和健壮性，在全球互联网领域炙手可热。
   之前一直记不住这个算法的名字，后来我想了想，它为什么叫这个名字？首先，协同，说明这个算法是不能孤立存在的，就像它
里面推荐给用户的商品，都是无法脱离该用户对除此之外的商品的评价以及其他用户对商品的评价，也即商品之间的评价，协同合作，
给出最后的评价。那么这里的过滤是什么意思呢，这里我理解成，通过已有数据，过滤掉难以分析的特征值，过滤同类，留下用户最想
看到的，最想用的元素。
    在这里，我先想直接说算法的代码，而不是这其中的原理和数学思想，之后我们再来说为什么要用那些数学公式或者思想。
    这是我们的数据集：
    <pre>
1 11 1
1 12 1
1 13 1
1 14 0
1 15 1
2 11 1
2 12 2
2 13 2
2 14 5
2 15 4
3 11 2
3 12 3
3 13 1
3 14 5
3 15 1
4 11 1
4 12 2
4 13 2
4 14 5
4 15 4
5 11 1
5 12 1
5 13 1
5 14 5
5 15 1
 </pre>
 
因为用户和商品的命名如果涉及到中英文名字，不好处理，这里我们第一列用1、2、3、4、5来分别代表用户的名字，用第二列来表示某一种
商品的名称，第三列自然是用户对商品的评价分了，以下是网上找的代码，结合了我对源码的分析：

```scala

import org.apache.spark.mllib.recommendation.{ALS, Rating}
import org.apache.spark.{SparkConf, SparkContext}

/**
  * Created by 2281444815 on 2016/12/2.
  */
object ALSTest {
  def main(args: Array[String]): Unit = {
    val conf = new SparkConf().setMaster("local").setAppName("ALSTest");
    val sc = new SparkContext(conf);
    val data = sc.textFile("D:\\data\\alstest.txt");
    val ratings = data.map(_.split(" ") match {
      case Array(user,item,rate) =>    //转化数据集
        Rating(user.toInt,item.toInt,rate.toDouble)  //将数据转化为专用Rating
    })
    val rank = 2;//秩，这部分会在之后解释
    val numIterations = 4;
    val model = ALS.train(ratings,rank,numIterations,0.01); //进行模型训练，0.01为正则化参数，防止过拟合

    //为用户1推荐一个物品
    val rs = model.recommendProducts(1,1);
    rs.foreach(println)

   //预测结果
    //从 ratings 中获得只包含用户和商品的数据集
    val usersProducts = ratings.map{
      case Rating(user,product,rate)=>
        (user,product)
    }

    /**使用推荐模型对用户商品进行预测评分，得到预测评分的数据集
      * 这里面的predict方法，输入： 获得一组已知的用户和物品的数据对，返回: 数据对和预测的评分组成的对应关系
      * Predict the rating of many users for many products.
      * The output RDD has an element per each element in the input RDD (including all duplicates)
      * unless a user or product is missing in the training set.
      *
      * @param usersProducts  RDD of (user, product) pairs.
      * @return RDD of Ratings.
      */
    val predictions = model.predict(usersProducts).map{
      case Rating(user,product,rate) =>
        ((user,product),rate)
    }

    /**join方法，将真实评分数据集与预测评分数据集进行合并
      * 即将原本数据中用户物品数据对与评分组成的对应关系，加入到预测的对应关系中，相当于最终的数据集中
      * 用两个部分，一个是真实的用户物品与评分数据，一个是预测的用户物品与评分数据
      * Return an RDD containing all pairs of elements with matching keys in `this` and `other`. Each
      * pair of elements will be returned as a (k, (v1, v2)) tuple, where (k, v1) is in `this` and
      * (k, v2) is in `other`. Uses the given Partitioner to partition the output RDD.
      */
    val ratesAndPres = ratings.map{
      case Rating(user,product,rate)=>
        ((user,product),rate)
    }.join(predictions)

   //计算均方差
    val MSE = ratesAndPres.map{
      case ((user,product),(r1,r2))=>
        val err = (r1-r2)
        err*err
    }.mean()
    println("Mean Squared Error = "+MSE)
  }
}
```

部分问题我已经在上面的代码里解释过，但还是有很多东西需要进行进一步解释：
（1）隐藏因子rank的值，很多教程中对该值草草带过，没有进行任何的说明，而我发现，这个值对最后的预测结果有着很重大的影响，
我会在之后解释这个值的意思。
（2）如果我们反复的运行这个程序，让它推荐用户1一件物品，你会惊讶的发现，会有两个结果，即使你将迭代的次数增加，
还是会出现两个结果，这也是我在数据集中为什么将 用户1对其他四件物品的评分设为1，一件物品的评分设为0；而用户5的四件物品评分设为1，
一件物品的评分设为5的原因，我会在之后说明我的用意。
综合上述的两个问题，这时候，我们有必要，也是必须来引入ALS算法中的数学思想了,我从网上摘下了对其中数学思想解释的部分内容：
![](https://github.com/woshidandan/hadoop-spark/blob/master/picture/als1.jpg)
![](https://github.com/woshidandan/hadoop-spark/blob/master/picture/als2.jpg)
在这张图片中，我们可以看到，我们有三个特征值（我们人为给它划分的）：性格、文化程度、兴趣爱好，这三个值，也即对应了我们模型训练中，隐藏因子rank的值，当然，我们上面的代码中，为了测试方面，将特征值的数量设为了2。这也是我经过学习之后，发现该算法的一个不足之处，因为在实际的学习或者实践中，
我们很难确定我们的特征值的数量，有的人看电影可能只是看特效给评分，有的人看电影，会综合电影的很多因素给出评分，而我们的算法，力求考虑到
整个数据集考虑到的方方面面，来对我们的个体进行分析，这样很可能造成以全概偏，对最后的结果造成严重的误差，所以，如何确定特征值rank的大小，
是要根据经验而来，还是根据我们的数据而来，又是怎么来的，我不得而知，只能通过将来的学习来了解这个问题的解决思路。
 
![](https://github.com/woshidandan/hadoop-spark/blob/master/picture/als3.jpg)
![](https://github.com/woshidandan/hadoop-spark/blob/master/picture/als4.jpg)

这里面所用到的数学思想，我们可以简化成这样：通过求偏导得出x，y的值，也就是，固定x轴求y，然后再通过得到的y来固定y轴，求x，反复迭代优化，
得到最终的x和y的值，也就是我们人为划分的特征值组成的特征向量矩阵，有了这个矩阵，我们就可以针对某一用户向他推荐物品。
在这个过程中，我们又涉及到我们在文章开头所说的“协同”了，在哪里体现我们的协同呢，就是在我们求x，y值的时候，很多时候，比如当我们固定了x轴求
y的时候，我们的y没有值，这时候我们应该根据整个数据集，或者寻找到与我们这组数据相似的那组值，来求我们的y，简单来说，就是有个爸爸给出了他对
几个电影的评分，而和爸爸性格相似的儿子，给出的电影评分几乎和爸爸一样，但是唯独少了对某一部电影的评分，这时候我们可以近似的将爸爸对这部电影
的评分，嫁接到儿子对这部电影的评分。
然后我们再来看看我上文中提到的，对测试数据的设置：
1 11 1      5 11 1
1 12 1      5 12 1
1 13 1      5 13 1
1 14 0      5 14 5
1 15 1      5 15 1
如果按照我们的主观判断，推荐给用户1一个物品，是不可能给他推荐14号物品的，因为他对它的评分为0（或者没有评分），而是在我们其余四件中选择一件推
荐给用户1，但是最后的测试结果呢：
 Rating(1,14,0.9243941809475156)
Mean Squared Error = 0.30299670467583506

 Rating(1,15,1.1868446679098792)
Mean Squared Error = 0.19951470222129394，

一方面我们多次运行，结果会不止一种，另一方面，竟然给我们推荐了物品14，而且就算是从其他四件中选一件，也只选了15号物品，相信大家看到这里，也很明白
我对用户5数据设置的用意了，在算法的运行过程中，会进行用户相似度计算，最小二乘法，相当于1号用户是儿子，5号用户是爸爸，因为1号与5号过于相似，因此
给它推荐了14号物品，而为什么给他又可能推荐了15号物品呢，这是因为数据集中还有其他用户，为了达到最后的推荐效果，同样做了相似度计算，因为对x，y的求解过程中，x，y一开始是随机化产生的，所以会造成了结果在几个值之间的不确定性。
在这里我们又引申出来另一个问题，那就是用户相似度的计算，代码中的思想是用到了最小二乘法，与其有关的算法有欧几里得相似度算法、余弦相似度算法，在这里因为篇幅的原因，我不再赘述，会在今后的文章中，再作进一步的阐述。
至此，协同过滤算法中，基本上所有的关键部分我们都有所了解和学习了，至于更深层次的内容，我也不想让自己过于纠结，毕竟知识水平有限，就交给今后的自己吧。同时也认识到，我们机器学习或者数据挖掘的很多算法，即使历经了几代前辈的修改优化，还是有不尽如意的地方，当然，这也是我们今后学习的价值所在。
