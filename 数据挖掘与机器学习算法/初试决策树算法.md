常用的数据分类方法，除了我们之前博客中介绍的K-means以及逻辑回归的方法外，还有一种比较有效和常用的方法：决策树。

决策树作为一种监督学习的算法，非常突出的优点是程序设计人员和使用者不需要掌握大量的基础知识和相关内容，计算可以
自行归纳完成，我们可以指定其生成树的高度，分类的数量等等，后面我们会有具体代码案例可以看到。任何一个只要符合
key-value模式的分类数据都可以根据决策树进行推断，应用十分的广泛。
首先我们来看一段问答：
问：小何是男的么？
答：是
问：小何帅么？
答：帅
问：小何是否婚配？
答：否
如果我们妹子希望通过一段问答，与我们的小何确定能否把这条单身狗收服，那么我们心里的活动可能如下：

![](https://github.com/woshidandan/hadoop-spark/blob/master/picture/jcs5.jpg)

 我们可以看到，在这段对话中，该妹子把一个特别复杂难以决断的问题：“我是不是要嫁这只单身汪”转化为
几个特征问题，每个特征问题的答案都会使妹子有一个简单的抉择，把问题一步一步的简单化，最终将整个问题解决。

这幅图，从某种意义上来说，也是一个决策树，但因其没有量化其中的判定条件，所以不是正规的决策树。

我们来看几个关于决策树的正规定义：
决策树（decision tree）是一个树结构（可以是二叉树或非二叉树）。其每个非叶节点表示一个特征属性上的测试，
每个分支代表这个特征属性在某个值域上的输出，而每个叶节点存放一个类别。使用决策树进行决策的过程就是从根节
点开始，测试待分类项中相应的特征属性，并按照其值选择输出分支，直到到达叶子节点，将叶子节点存放的类别作为
决策结果。

![](https://github.com/woshidandan/hadoop-spark/blob/master/picture/jcs.jpg)

因为网上此类资料过多，大多都是相同的，并且对其中的几个重要的地方没有解释，在我们开始正式学习其中算法思想前，
我们首先明确下面几个定义：
熵：
我个人的理解，就是事物的复杂度，难以处理的程度，熵越大，事物的复杂度越大，不同属性具有不同的熵。
自然界中，大部分事物都是一个熵增的过程，我们以前在高中也学习过：比如理想气体扩散后不可能自己缩回去，温度只能
自发从高温传到低温，这些都是熵增的过程。一句话，不可逆过程。
属性的连续性：
大部分本身存在的属性，都不是间断的，而是我们人将其连续性通过某种手段，分段并量化，简单来说，我们把从1到10的
连续数字，将其分为两类，一类是大数字，一类是小数字，这里的两类，就是人为划分的。

那么问题来了，我们如何计算一个事物属性所包含的熵呢？

![](https://github.com/woshidandan/hadoop-spark/blob/master/picture/jcs1.jpg)

这里我们设计到一个算法：ID3算法，ID3算法是基于信息熵的一种经典决策树构建算法，它是一种贪心算法(总是选择增益最大的)，
用来构造决策树。ID3算法起源于概念学习系统(CLS)，以信息熵的下降速度为选取测试属性的标准，即在每个节点选取还尚未被用来
划分的、具有最高信息增益的属性作为划分标准，然后继续这个过程，直到生成的决策树能完美分类训练样例。可以说，ID3算法的核
心就是信息增益的计算。
这里我将信息增益理解为：下一步的举措，对于减少问题的复杂度所做出的贡献，比如上面的妹子案例中，妹子的每一个问题都可以
看作一个属性，每个问题问完后，妹子对于自己最终做出的选择又更加的确信一步。

下面我们来看一个案例

![](https://github.com/woshidandan/hadoop-spark/blob/master/picture/jcs2.jpg)

相比较我们前几次学习的公式来说，这个公式要简单很多，总的来说，我概括为下面一段话：
属性的每个取值，在该取值中，所对应的分类结果有多种，在该取值的条件下计算其中对应的分类结果的熵，与该取值在该属性所占比例相乘。
其中，Gain(S,A)是由于知道属性A的值而导致的期望熵减少，即该问题分类时的复杂性减小。
在什么情况下，我们的信息熵是最大的呢？我们之前也说了，当问题最复杂时，最难以抉择时，熵最大。比如我们有两个队伍比赛，甲乙获胜的
概率分别是p和1-p，使用信息熵计算可以得到任何一方获胜的几率为-(plogp+(1-p)log(1-p))，我们可以通过简单的求导得出：当p为1/2时，
熵取得最大值为2，即双方获胜可能一样，不知道谁会最终取得获胜时熵最大。这也证实了我们之前的言论。

![](https://github.com/woshidandan/hadoop-spark/blob/master/picture/jcs3.jpg)
![](https://github.com/woshidandan/hadoop-spark/blob/master/picture/jcs6.jpg)

当然，在整个过程中，简化了我们属性的特征，即将本来是连续属性的日志密度和好友密度，离散化处理了。
对于特征属性为连续值，可以如此使用ID3算法：先将D中元素按照特征属性排序，则每两个相邻元素的中间点可以看做潜在分裂点，从第一个潜
在分裂点开始，分裂D并计算两个集合的期望信息，具有最小期望信息的点称为这个属性的最佳分裂点，其信息期望作为此属性的信息期望。

另外，ID3也存在下面这个问题，也就是一个属性只有一中唯一的可能：

![](https://github.com/woshidandan/hadoop-spark/blob/master/picture/jcs4.jpg)

关于决策树的理论我们也暂时学到这里了，下面我们进入spark的案例实践：
我们就使用上述的检测账号是否真实的案例，但是对其中的s、m、l小中大我们分别赋值：1、2、3,得到以下数据：（数据格式以前案例中已做说明）

