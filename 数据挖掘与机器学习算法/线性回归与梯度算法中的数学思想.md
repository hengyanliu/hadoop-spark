  回归分析是一种用来确定两种或两种以上变量间相互依赖的定量关系的统计分析方法，运用十分广泛。我在学习时的一点见解，带给大家，
希望能帮助大家理解该算法中的数学思想，这对于我们后续的算法实战很重要。
 这一篇可能写的比较乱，是根据所学的教程，说的都是这个算法中自己以前不明白的地方，涉及到的方面比较多。
To perform supervised learning, we must decide how we’re going to represent functions/hypotheses h in a computer. 
As an initial choice, lets say we decide to approximate y as a linear function of x:

![](https://github.com/woshidandan/hadoop-spark/blob/master/picture/sgd1.png)

 Now, given a training set, how do we pick, or learn, the parameters θ?
One reasonable method seems to be to make h(x) close to y, at least for the training examples we have. 
To formalize this, we will define a function that measures, for each value of the θ’s, how close the h(x (i) )’s 
are to the corresponding y (i) ’s. We define the cost function:

![](https://github.com/woshidandan/hadoop-spark/blob/master/picture/sgd2.png)

这其中J(θ)的值相当于我们的假设与我们的实际差距有多大，每个点的差距合起来，即为衡量差距大小的J(θ)的值。
We want to choose θ so as to minimize J(θ). To do so, lets use a search
algorithm that starts with some “initial guess” for θ（通常这个初值我们可以取0）, and that repeatedly changes θ to 
make J(θ) smaller, until hopefully we converge to a value ofθ that minimizes J(θ). 
Specifically, lets consider the gradient descent algorithm, which starts with some initial θ,
and repeatedly performs the update: 

![](https://github.com/woshidandan/hadoop-spark/blob/master/picture/sgd3.png)

This update is simultaneously performed for all values of j = 0,...,n.)
Here, α is called the learning rate. This is a very natural algorithm that
repeatedly takes a step in the direction of steepest decrease of J.
这在里我想先说的是，为什么我们求偏导就能更新，就能最后得到我们的θj的值，为什么我们会有这个公式，为什么这个公式的存在是合理的？
在网络上我找到这些：

![](https://github.com/woshidandan/hadoop-spark/blob/master/picture/sgd4.png)

![](https://github.com/woshidandan/hadoop-spark/blob/master/picture/sgd5.png)

也就是说这个公式确实是合理的，那姑且就当做这里默认的吧，等以后进一步的学习，再来解决这个问题。
通过不断更新θj，使J(θ)取得最小值，这里公式所用到的赋值方法，是类似于我们c语言中的赋值方法，利用右边的值，迭代覆盖左边的值。
这里的θj不断变化，参数更新，同时又影响到我们的J(θ)函数，力求取到最优的参数θj的值，在这个过程中学习速度，也就是梯度下降的速
度α（表示步长，也就是每次按照梯度减少的方向变化多少）的值不断的减小，最终趋向于0，使得θj的值最终趋于稳定，在我们只有一个数
据时，这个式子还可以进一步化简，得到：
   
In order to implement this algorithm, we have to work out what is the
partial derivative term on the right hand side. Lets first work it out for the
case of if we have only one training example (x,y), so that we can neglect
the sum in the definition of J. We have:

![](https://github.com/woshidandan/hadoop-spark/blob/master/picture/sgd6.png)

For a single training example, this gives the update rule: 

![](https://github.com/woshidandan/hadoop-spark/blob/master/picture/sgd7.png)

The rule is called the LMS update rule (LMS stands for “least mean squares”),
and is also known as the Widrow-Hoff learning rule。
这个方法叫做最小均方，观察我们最终得到的这个公式，可以看出，求出的θj的值，同时也影响到我们函数y的本身，这也是为何在我们的数
据集不止一个数据时，会造成我们梯度下降的原因，就像图中所示：

![](https://github.com/woshidandan/hadoop-spark/blob/master/picture/sgd8.png)

站在山顶，通过沿着下降最快的方向下山。
在数据不止一组时，公式变为：
We’d derived the LMS rule for when there was only a single trainingexample. There are two ways to modify this method
for a training set ofmore than one example. The first is replace it with the following algorithm:
Repeat until convergence

![](https://github.com/woshidandan/hadoop-spark/blob/master/picture/sgd9.png)

This method looks at every example in the entire training set on every step, and is called batch gradient descent.
  这个算法叫做批度下降算法，其缺点是，在它每到达一个点，也就是下山每到一个地方，因为它需要使用整个数据集来迭代。


  为此，我们引入随机梯度下降算法：
In this algorithm, we repeatedly run through the training set, and each timewe encounter a training example, we update
the parameters according tothe gradient of the error with respect to that single training example only.
This algorithm is called stochastic gradient descent (also incremental gradient descent).

![](https://github.com/woshidandan/hadoop-spark/blob/master/picture/sgd10.png)

该算法训练的速度要比批度下降算法快的多，但是因为我们每次到达一个点，只使用到了数据集中的一个数据，因此最后的结果，我们无法达到全局
的最小值，只能在最小值附近徘徊。如下图中的黑线所示：

![](https://github.com/woshidandan/hadoop-spark/blob/master/picture/sgd11.png)

我们知道，任何一个模型都是存在误差的，我们的这个算法也同样如此，为什么它的存在是合理的，我们可以对其中误差的概率性进行探讨：
When faced with a regression problem, why might linear regression, andspecifically why might the least-squares cost
function J, be a reasonablechoice? In this section, we will give a set of probabilistic assumptions, under
which least-squares regression is derived as a very natural algorithm.Let us assume that the target variables 
and the inputs are related via the Equation

![](https://github.com/woshidandan/hadoop-spark/blob/master/picture/sgd12.png)

误差使用高斯公式并替换其中的参数：

![](https://github.com/woshidandan/hadoop-spark/blob/master/picture/sgd13.png)

![](https://github.com/woshidandan/hadoop-spark/blob/master/picture/sgd14.png)

综合以上的两个公式，我们的误差越小，概率越大，即数据模型实现对样本的拟合程度越大，因此，我们需要将概率的值提升到最大。

在多组数据时，利用极大似然估计法来统计误差：

![](https://github.com/woshidandan/hadoop-spark/blob/master/picture/sgd15.png)

首先是为何我们可以选择极大似然估计法来统计？
【参数的似然性和数据的概率】
在数理统计学中，似然函数是一种关于统计模型中的参数的函数，表示模型参数中的似然性。 
似然函数在统计推断中有重大作用，如在最大似然估计和费雪信息之中的应用等等。“似然性”与“或然性”或“概率”意思相近，都是指某种事件发生
的可能性，但是在统计学中，“似然性”和“或然性”或“概率”又有明确的区分。 概率用于在已知一些参数的情况下，预测接下来的观测所得到的结果，
而 似然性 则是用于在已知某些观测所得到的结果时，对有关事物的性质的参数进行估计。

举例如下：
对于“一枚正反对称的硬币上抛十次”这种事件，我们可以问硬币落地时十次都是正面向上的“概率”是多少；
而对于“一枚硬币上抛十次，落地都是正面向上”这种事件，我们则可以问，这枚硬币正反面对称（也就是正反面概率均为0.5的概率）的“似然”程度是多少。
最大似然估计是在给定模型（含有未知参数）和样本集的情况下，用来估计模型参数的方法。其基本思想是找到最佳的模型参数，使得模型实现对样本的
最大程度拟合，也就使样本集出现的可能性最大

基本思想：
这一方法是基于这样的思想：我们所估计的模型参数，要使得产生这个给定样本的可能性最大。在最大释然估计中，我们试图在给定模型的情况下，找到
最佳的参数，使得这组样本出现的可能性最大。举个极端的反面例子，如果我们得到一个中国人口的样本，男女比例为3:2，现在让你估计全国人口的真
实比例，你肯定不会估计为男：女=1:0。因为如果是1:0，不可能得到3:2的样本。我们大多很容易也估计为3:2，为什么？样本估计总体？其背后的思想
其实最是最大似然。在机器学习的异常检测中，根据模型（通过学习得来的）计算一个数据点出现的概率，如果这个概率小于某个我们事先设定的值，就把
它判为异常。



然后，因为以前概率统计没好好学，为什么用极大似然求解时，是连乘？我们可以想象，就像是一个事件：取2个红球，3个黑球，每取一次都是独立的，那
么我们这个事件的概率就是p（红）* p（红）* p（黑）* p（黑）* p（黑），在我们上面的那个问题中，同样存在这种类似情况，L(θ)即为组成其事件的
每一点的概率相乘。

由上面我们已经得到

![](https://github.com/woshidandan/hadoop-spark/blob/master/picture/sgd15.png)

因此，我们必须使这个值，大于某个设定的值时，我们才说，我们的模型是有意义的，将这个L(θ)取对数（不影响其大小，为了计算方便）：

![](https://github.com/woshidandan/hadoop-spark/blob/master/picture/sgd17.png)

最后的结果，我们可以看出，要想使ℓ(θ)越大，必须使（1）越小，而（1）刚好对应了我们的J(θ)，即J(θ)会越小，也就是hθ (x (i) ) − y (i))的差值误
差越小，刚好与我们之前的梯度下降算法的思想一致。

  当然，这部分的Spark实战部分，我也会在我的博客中更新，欢迎关注。

  如果你觉得我的文章对你有帮助，欢迎到我的[博客](http://xiaohegithub.cn/)留言区留言交流，我会虚心听取大家的意见和建议，为进一</br>
步的学习做调整。更多的算法解析，我也会根据自己的学习在我的博客发布，欢迎来访www.xiaohegithub.cn</br>
                                                                  2017/1/22
