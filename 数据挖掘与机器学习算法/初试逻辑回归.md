   

   逻辑回归和线性回归类似，但是它不属于回归分析家族，而是分类家族，也是无监督学习的一个重要的算法。
逻辑回归是目前数据挖掘和机器学习领域中使用较为广泛的一种对数据进行处理的算法，一般用于对某些数据或
事物的归属及可能性进行评估。目前较为广泛地应用在流行病学中，比较常用的情形是探索某疾病的危险因素，
根据危险因素预测某疾病发生的概率等。
</br>
![](https://github.com/woshidandan/hadoop-spark/blob/master/picture/logistics1.png)
</br>
   其他方面不再赘述，直接给出一元逻辑回归的简单案例：

```scala
import org.apache.spark.mllib.classification.{LogisticRegressionWithSGD, SVMWithSGD}
import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.regression.LabeledPoint
import org.apache.spark.{SparkConf, SparkContext}

/**测试数据：
1,2
1,3
1,4
1,5
1,6
0,7
0,8
0,9
0,10
0,11
  * Created by 2281444815 on 2017/1/24.
  */
object LogisticRegression {
  val conf = new SparkConf().setMaster("local").setAppName("LogisticRegression");
  val sc = new SparkContext(conf);

  def main(args: Array[String]): Unit = {
    val data = sc.textFile("D://data//logisticregression.txt");
    val parsedData = data.map{ line =>
      val parts = line.split(",")
      //注意这里即使x只有一个数据，也不可省去split(" ")方法
      LabeledPoint(parts(0).toDouble,   Vectors.dense(parts(1).split(" ").map(_.toDouble)));
    }.cache();

    val model = LogisticRegressionWithSGD.train(parsedData,50);
    val result = model.predict(Vectors.dense(16)); //对16进行预测
    println(result);
    //预测结果为：0.0
  }
}
```

   其次，是贝叶斯方法，它作为统计分析中最基本的数据分析方法，这种方法是基于假设的先验概率、给定假设下
观察到不同数据的概率，以及观察到的数据本身而得出的。其方法为，将关于未知参数的先验信息与样本信息综合，
再根据贝叶斯公式，得出后验信息，然后根据后验信息去推断未知参数的方法。
贝叶斯定理：
</br>
![](https://github.com/woshidandan/hadoop-spark/blob/master/picture/logistics2.png)
</br>
   相信这个公式大家都很熟悉。简单来说，以前我们求的概率是，我们男生穿裤子的概率是多少，现在通过贝叶斯定
理求的是穿裤子的是男生的概率是多少，前者叫做“先验概率”，后者叫做“后验概率”。
   关于原理，我们可通过下图来进一步理解：
</br>
![](https://github.com/woshidandan/hadoop-spark/blob/master/picture/logistics3.png)
</br>
   值得注意的是，在最后的这个连乘公式中，如果在我们的实践中，比如垃圾邮件的分类中，若我们的特征属性为若
干个单词或者词语，那么我们的各个特征不可能是独立的，该公式也是不正确的，比如有个推销保险的邮件，那么“购买”
与“保险”这两个词组的出现必然是相连的，或者是有关系的，也即“购买”的出现，影响了“保险”的出现，当然在这里为
了研究的方面，我们不考虑这些。
    当然，我们注意到，这里的x被划分为若干个特征属性，在实际案例中，很多值时连续的，我们可以通过一些方法将
其离散化：
</br>
![](https://github.com/woshidandan/hadoop-spark/blob/master/picture/logistics4.png)
</br>
    另一个需要讨论的问题就是当P(a|y)=0即其中的某个特征出现的概率为0怎么办，当某个类别下某个特征项划分没有
出现时，就是产生这种现象，这会令分类器质量大大降低。为了解决这个问题，我们引入Laplace校准，它的思想非常简单，
就是对没类别下所有划分的计数加1，这样如果训练样本集数量充分大时，并不会对结果产生影响，并且解决了上述频率为0
的尴尬局面。通俗来说，对公式的分子和分母同时加上常数，使其避免出现0除以0的错误。
    简单分析了其中的数学原理，下面是一个案例的实战：

```scala
import org.apache.spark.mllib.classification.NaiveBayes
import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.mllib.util.MLUtils
/**
我们有以下数据：
0,1 0 0
0,2 0 2
1,0 1 0
1,0 2 0
2,0 0 1
2,0 0 2
其中第一列是分类的结果，后续几列是特征值。

  * Created by 2281444815 on 2017/2/7.
  */
object Bayes {
  def main(args: Array[String]): Unit = {

    val conf = new SparkConf().setMaster("local").setAppName("Bayes");
    val sc = new SparkContext(conf);

    val data = MLUtils.loadLabeledPoints(sc, "D://data//bayes.txt");
    //这里我们采用系统提供的数据格式方法导入
    val model = NaiveBayes.train(data, 1.0) //注意别导错包啊，要不然错误要把你找死。。。

    model.labels.foreach(println);
    model.pi.foreach(println);

    val test = Vectors.dense(0, 0, 10);
    val result = model.predict(test);
    println("预测结果是" + result);
  }
}
```

结果：
</br>
![](https://github.com/woshidandan/hadoop-spark/blob/master/picture/logistics5.png)
</br>
<pre>
    至于网上还有很多的类似实战案例，在这里就不提供代码了，基本类似，但是我想说一下基于“微博僵尸粉鉴定”
的这个案例中，某些手段和方法。首先鉴定的结果分为是和不是，在我们的代码中即标记为0和1，那么我们得到的这
些用户的特征，又怎么划分和数据化呢，或者说怎么样得到我们的特征值？
    这里，比如，我们可以用注册的天数除以已发微博的数目得到一个数值a，当a大于某个值时，是一个标准，当a
小于某个值的时候又是另一个标准；同样，类似的我们可以用其是否注册信息完整，是否有手机号绑定等作为另一个
特征值的值，由此，我们便将得到的数据抽象出来，称为我们可以利用的数据，当然，在这个过程中大量的数据清洗
和处理是少不了的。
    朴素贝叶斯目前常用于文本分类工作，模型简单，运行速度快，被广泛应用在实际生活中，分类的结果也较为理想。
当然，贝叶斯还有很多需要深入的地方，这些留给后续的深入学习。
    本文部分理论内容来源于网络，如有异议，请及时与我联系。 
如果你觉得我的文章对你有帮助，欢迎到我的[博客](http://xiaohegithub.cn/)留言区留言交流，我会虚心听取大家的意见和建议，为进一</br>
步的学习做调整。更多的算法解析，我也会根据自己的学习在我的博客发布，欢迎来访www.xiaohegithub.cn</br>
                                 2017/2/6
                                 </pre>
